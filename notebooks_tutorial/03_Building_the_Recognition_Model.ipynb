{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "%matplotlib inline\n",
    "# Open a new thread to manage the external cv2 interaction\n",
    "cv2.startWindowThread()\n",
    "\n",
    "def plt_show(image, title=\"\"):\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.imshow(image, cmap=\"Greys_r\")\n",
    "    plt.show()\n",
    "    \n",
    "class FaceDetector(object):\n",
    "    def __init__(self, xml_path):\n",
    "        self.classifier = cv2.CascadeClassifier(xml_path)\n",
    "    \n",
    "    def detect(self, image, biggest_only=True):\n",
    "        scale_factor = 1.2\n",
    "        min_neighbors = 5\n",
    "        min_size = (30, 30)\n",
    "        biggest_only = True\n",
    "        flags = cv2.CASCADE_FIND_BIGGEST_OBJECT | \\\n",
    "                    cv2.CASCADE_DO_ROUGH_SEARCH if biggest_only else \\\n",
    "                    cv2.CASCADE_SCALE_IMAGE\n",
    "        faces_coord = self.classifier.detectMultiScale(image,\n",
    "                                                       scaleFactor=scale_factor,\n",
    "                                                       minNeighbors=min_neighbors,\n",
    "                                                       minSize=min_size,\n",
    "                                                       flags=flags)\n",
    "        return faces_coord\n",
    "    \n",
    "class VideoCamera(object):\n",
    "    def __init__(self, index=0):\n",
    "        self.video = cv2.VideoCapture(index)\n",
    "        self.index = index\n",
    "        print self.video.isOpened()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.video.release()\n",
    "    \n",
    "    def get_frame(self, in_grayscale=False):\n",
    "        _, frame = self.video.read()\n",
    "        if in_grayscale:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        return frame\n",
    "\n",
    "def cut_faces(image, faces_coord):\n",
    "    faces = []\n",
    "    \n",
    "    for (x, y, w, h) in faces_coord:\n",
    "        w_rm = int(0.3 * w / 2)\n",
    "        faces.append(image[y: y + h, x + w_rm: x + w - w_rm])\n",
    "         \n",
    "    return faces\n",
    "\n",
    "def normalize_intensity(images):\n",
    "    images_norm = []\n",
    "    for image in images:\n",
    "        is_color = len(image.shape) == 3 \n",
    "        if is_color:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        images_norm.append(cv2.equalizeHist(image))\n",
    "    return images_norm\n",
    "\n",
    "def resize(images, size=(50, 50)):\n",
    "    images_norm = []\n",
    "    for image in images:\n",
    "        if image.shape < size:\n",
    "            image_norm = cv2.resize(image, size, \n",
    "                                    interpolation=cv2.INTER_AREA)\n",
    "        else:\n",
    "            image_norm = cv2.resize(image, size, \n",
    "                                    interpolation=cv2.INTER_CUBIC)\n",
    "        images_norm.append(image_norm)\n",
    "\n",
    "    return images_norm\n",
    "\n",
    "def normalize_faces(frame, faces_coord):\n",
    "    faces = cut_faces(frame, faces_coord)\n",
    "    faces = normalize_intensity(faces)\n",
    "    faces = resize(faces)\n",
    "    return faces\n",
    "\n",
    "def draw_rectangle(image, coords):\n",
    "    for (x, y, w, h) in coords:\n",
    "        w_rm = int(0.2 * w / 2) \n",
    "        cv2.rectangle(image, (x + w_rm, y), (x + w - w_rm, y + h), \n",
    "                              (150, 150, 0), 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img style=\"float: right; margin-right: 100px\" src=\"http://www.apulus.com/wp-content/uploads/2014/11/OpenCV-Logo.png\">\n",
    "\n",
    "<h1 style=\"align: center; color: \">Building the Recognition Model</h1>\n",
    "<br>\n",
    "\n",
    "Three type of models:\n",
    "- Eigen Faces <br>\n",
    "`cv2.face.createEigenFaceRecognizer()`\n",
    "- Fisher Faces<br>\n",
    "`cv2.face.createFisherFaceRecognizer()`\n",
    "- LBPH Faces<br>\n",
    "`cv2.face.createLBPHFaceRecognizer()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\" style='color: #be2830'>Eigen Faces</h2>\n",
    "\n",
    "- A $100 \\times 100$ pixel image lies in a $10,000$ dimensional space.\n",
    "- Not all dimensions are usefull\n",
    "- Principal Component Analysis (PCA)\n",
    "- A few dimensions account for most of the information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How does it work?\n",
    "\n",
    "- Finds the PCA subspace by eigenvalue decomposition: \n",
    "  - Example: Dataset of 400 images of $100 \\times 100$, $S=XX^T$ where $X=10000\\times 400$, then $S=10000 \\times 10000$ matrix. Instead from linear algebra eigenvalue decomposition we take $S=X^TX$ instead. Then recover original eigenvector by multiplying by $X$.\n",
    "- Projects all training samples into the PCA subspace.\n",
    "- Projects the sample image into the PCA subspace.\n",
    "- Finds the nearest neighbor between the projected training images and the sample image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"float: left; width: 50%\">\n",
    "Reconstruction\n",
    "<img style=\"width: 90%\" src=\"http://docs.opencv.org/2.4/_images/eigenface_reconstruction_opencv.png\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%\">\n",
    "Eigenvectors\n",
    "<img style=\"width: 100%\" src=\"http://docs.opencv.org/2.4/_images/eigenfaces_opencv.png\">\n",
    "</div>\n",
    "<p>Encode facial features but also illumination.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\" style='color: #be2830'>Fisher Faces</h2>\n",
    "\n",
    "- Eigenfaces doesn't consider any classes. \n",
    "    - Discriminative information may be lost.\n",
    "    - Variance can come from external sources, such as light.\n",
    "- Linear Descriminat Analysis (LDA), Sir. R. A. Fisher, 1936\n",
    "- Combination of features that best separated classes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How does it work?\n",
    "\n",
    "- Same classes should cluster together.\n",
    "- Different classes are as far as possible from each other.\n",
    "- Peforms class-specific dimensionality reduction.\n",
    "- Find combination of features that best separates between classes.\n",
    "- If sample number < input data dimension,  then PCA first.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"float: left; width: 50%\">\n",
    "Type of Reconstruction\n",
    "<img style=\"width: 90%\" src=\"http://docs.opencv.org/2.4/_images/fisherface_reconstruction_opencv.png\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%\">\n",
    "Fisher Eigenvectors\n",
    "<img style=\"width: 90%\" src=\"http://docs.opencv.org/2.4/_images/fisherfaces_opencv.png\">\n",
    "</div>\n",
    "<p>Does't depend on illumination as much as eigenfaces.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Performance\n",
    "<img style=\"width: 60%; float:right; margin-right: 20px\" src=\"http://docs.opencv.org/2.4/_images/at_database_small_sample_size.png\">\n",
    "<br>\n",
    "<br>\n",
    "Based on a fairly easy database: AT&T Face database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### AT&T Database\n",
    "\n",
    "<img style=\"width:100%\" src=\"images/AT&T.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\" style='color: #be2830'>LBPH Faces</h2>\n",
    "- Don't look at the whole image\n",
    "- Describe local features\n",
    "- Comparing each pixel with neighborhood\n",
    "- Local Binary Patters:\n",
    "<img style=\"margin-left: 200px\" src=\"http://docs.opencv.org/2.4/_images/lbp.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### How does it work?\n",
    "\n",
    "- Center pixel act as threshold for neighborhood\n",
    "- Align arbitrary number of neighbors on a circle, enables the capture of the following neighborhoods:\n",
    "<img style=\"margin-left: 200px\" src=\"http://docs.opencv.org/2.4/_images/patterns.png\">\n",
    "- Incorporate spatial information by Local Binary Patterns Histograms\n",
    "- Roboust againts monotonic grayscale transformations\n",
    "<img style=\"width: 40%; margin-left: 200px\" src=\"http://docs.opencv.org/2.4/_images/lbp_yale.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\" style='color: #be2830'>Training the Model</h2>\n",
    "```python\n",
    "recognizer.train(images, labels)\n",
    "```\n",
    "- images: list of numpy arrays\n",
    "- labels: numpy array with the labels corresponding to the images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def collect_dataset():\n",
    "    images = []\n",
    "    labels = []\n",
    "    labels_dic = {}\n",
    "    people = [person for person in os.listdir(\"people/\")]\n",
    "    for i, person in enumerate(people):\n",
    "        labels_dic[i] = person\n",
    "        for image in os.listdir(\"people/\" + person):\n",
    "            images.append(cv2.imread(\"people/\" + person + '/' + image, \n",
    "                                     0))\n",
    "            labels.append(i)\n",
    "    return (images, np.array(labels), labels_dic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Collect image data and train models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "images, labels, labels_dic = collect_dataset()\n",
    "\n",
    "rec_eig = cv2.face.createEigenFaceRecognizer()\n",
    "rec_eig.train(images, labels)\n",
    "\n",
    "# needs at least two people \n",
    "rec_fisher = cv2.face.createFisherFaceRecognizer()\n",
    "rec_fisher.train(images, labels)\n",
    "\n",
    "rec_lbph = cv2.face.createLBPHFaceRecognizer()\n",
    "rec_lbph.train(images, labels)\n",
    "\n",
    "print \"Models Trained Succesfully\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\" style='color: #be2830'>Making a Prediction</h2>\n",
    "<br>\n",
    "### For OpenCV 3.1.0\n",
    "``` python\n",
    "collector = cv2.face.MinDistancePredictCollector()\n",
    "recognizer.predict(image, collector)\n",
    "conf = collector.getDist()\n",
    "pred = collector.getLabel()\n",
    "\n",
    "```\n",
    "\n",
    "### For OpenCV 3.0.0\n",
    "``` python\n",
    "prediction, confidence = recognizer.predict(image)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Get a sample picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "webcam = VideoCamera()\n",
    "frame = webcam.get_frame()\n",
    "detector = FaceDetector(\"xml/frontal_face.xml\")\n",
    "frame = webcam.get_frame()\n",
    "faces_coord = detector.detect(frame)\n",
    "faces = normalize_faces(frame, faces_coord)\n",
    "face = faces[0]\n",
    "plt_show(face) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Make Predictions with the three models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "collector = cv2.face.MinDistancePredictCollector()\n",
    "\n",
    "rec_eig.predict(face, collector)\n",
    "conf = collector.getDist()\n",
    "pred = collector.getLabel()\n",
    "print \"Eigen Faces -> Prediction: \" + labels_dic[pred].capitalize() +\\\n",
    "\"    Confidence: \" + str(round(conf))\n",
    "\n",
    "rec_fisher.predict(face, collector)\n",
    "conf = collector.getDist()\n",
    "pred = collector.getLabel()\n",
    "print \"Fisher Faces -> Prediction: \" +\\\n",
    "labels_dic[pred].capitalize() + \"    Confidence: \" + str(round(conf))\n",
    "\n",
    "rec_lbph.predict(face, collector)\n",
    "conf = collector.getDist()\n",
    "pred = collector.getLabel()\n",
    "\n",
    "print \"LBPH Faces  -> Prediction: \" + labels_dic[pred].capitalize() +\\\n",
    "\"    Confidence: \" + str(round(conf)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NEXT\n",
    "<ol> \n",
    "    <h2> <li> Manipulation of Images and Videos. [DONE]</h2> \n",
    "    <h2> <li> Face Detection and Building the Dataset [DONE]</h2>\n",
    "    <h2> <li> Building the Recognition Model [DONE]</h2>\n",
    "    <h2 style='color: #be2830'><a style='color: #be2830' href=\"http://localhost:8888/notebooks/04_Recognize_Faces_in_a_Live_VIdeo_Feed.ipynb\"> <li>  Recognize Faces in a Live VIdeo Feed</a></h2>\n",
    "<ol>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
