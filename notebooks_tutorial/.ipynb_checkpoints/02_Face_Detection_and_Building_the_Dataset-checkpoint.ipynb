{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import clear_output # Extra\n",
    "\n",
    "%matplotlib inline\n",
    "# Open a new thread to manage the external cv2 interaction\n",
    "cv2.startWindowThread()\n",
    "\n",
    "def plt_show(image, title=\"\"):\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.imshow(image, cmap=\"Greys_r\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img style=\"float: right; margin-right: 200px\" src=\"http://www.apulus.com/wp-content/uploads/2014/11/OpenCV-Logo.png\">\n",
    "\n",
    "<h1 style=\"align: center; color: \">Face Detection using Haar Featured-based Cascade Classifiers</h1>\n",
    "<br>\n",
    "- Paul Viola and Michale Jones \"Rapid Object Detection using a Boosted Cascade of Simple Features\" (2001)\n",
    "- Haar Features\n",
    "- Integral Image\n",
    "- Adaboost\n",
    "- Cascading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Haar Features\n",
    "<div style=\"float: left; width: 50%; height: 300px\">\n",
    "<br>\n",
    "<ul>\n",
    " <li> Edge features\n",
    " <li>Line features\n",
    " <br/>\n",
    " <li>Center surround features\n",
    "</ul>\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%; height: 300px\">\n",
    "<img src=\"http://fileadmin.cs.lth.se/graphics/theses/projects/facerecognition/1_all_haar_wavelets.png\">\n",
    "</div>\n",
    "<br>\n",
    "<img style=\"margin: 0 auto; width: 60%\" src=\"http://answers.opencv.org/upfiles/14253135944291716.png\">\n",
    "<br>\n",
    "$$Output =  \\sum{P_{White}} - \\sum{P_{Black}} \\\\$$\n",
    "Output is high when regions are similar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Problems: \n",
    "1. Even a 24x24 window results over 160,000 features.\n",
    "2. Every time you apply one of these 160,000 you need to sum all pixel intensities.\n",
    "\n",
    "Solutions:\n",
    "1. Adaboost\n",
    "2. Integral Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Integral Image\n",
    "<div style=\"float: right; width: 40%\">\n",
    "<img src=\"http://www.nongnu.org/rapp/doc/rapp/integral.png\">\n",
    "</div>\n",
    "S = $\\sum{C}+\\sum{A}-\\sum{B}-\\sum{D}$<br>\n",
    "\n",
    "Idea:<br>\n",
    "Prior to applying features, convert each pixel intensity by the sum of all pixel intensities to the left and above it.\n",
    "<img style=\"margin-left: 150px\" src=\"images/integral_image.png\">\n",
    "\n",
    "<font style=\"color: #be2830\">Reduce to 4 numbers!</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Adaboost\n",
    "$$ F(x)=\\alpha_1f_1(x)+\\alpha_2f_2(x)+\\alpha_3f_3(x)+\\dots \\\\$$\n",
    "- Weak classifiers: $f_n(x) $\n",
    "- Strong classifier: $F(x) $\n",
    "- The bigger the weight $\\alpha$ the more relevant the featured is.\n",
    "\n",
    "Total amount of features: 160000+ <br>\n",
    "<font style=\"color: #be2830\">After Adaboost: 6000 features! </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Problem\n",
    "- Still 6000 features are too many\n",
    "- They need to sweep the hole image in all differen sizes.\n",
    "\n",
    "Solution\n",
    "- Cascading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cascading\n",
    "<div style=\"float: right; width: 50%\">\n",
    "<img style= \"width: 80%\" src=\"http://www.bogotobogo.com/python/OpenCV_Python/images/FaceDetection/stages.png\">\n",
    "</div>\n",
    "- Separate features in different classifiers\n",
    "- Discard if really sure\n",
    "- 38 stages with 1, 10, 25, 25 and 50 features in first 5 stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Can be applied for any type of object\n",
    "\n",
    "OpenCV comes with a trainer as well as detector. If you want to train your own classifier for any object like car, planes etc. you can use OpenCV to create one.\n",
    "\n",
    "<div style=\"width: 33%; float: left; height: 250px\">\n",
    "    <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSOofIjL3un1IgzTmuF-0C4kryjYeFxLUtV1mXrhDBXQhdgcJhp\">\n",
    "</div>\n",
    "<div style=\"width: 33%; float: right; height: 250px\">\n",
    "    <img src=\"https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcS4w56wb3RmNacBmu815ORfX2cLYgtrFsutW2_k0nHNrmcjo8_Y\">\n",
    "</div>\n",
    "<div style=\"width: 33%; margin: 0 auto; height: 250px\">\n",
    "    <img src=\"https://abhishek4273.files.wordpress.com/2014/03/output1.jpg\">\n",
    "</div>\n",
    "<div style=\"width: 33%; float: left; height: 250px\">\n",
    "    <img src=\"http://answers.opencv.org/upfiles/13610841248853756.png\">\n",
    "</div>\n",
    "<div style=\"width: 33%; float: right; height: 250px\">\n",
    "    <img src=\"https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcRV6hfNajOFN92rZVhxS7e_c9dcbKl-1psaCus4Gx71wtehWEYu\">\n",
    "</div>\n",
    "<div style=\"width: 33%; margin: 0 auto; height: 250px\">\n",
    "    <img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSY2QX_FSmiqiS_5akHP2RjXM9Dy3D7ocEdsyW0H27XGrkWvx0y\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "List of available pretraied models in OpenCV:\n",
    "\n",
    "<div style=\"float: left; width: 50%\">\n",
    "<ul>\n",
    "    <li> haarcascade_eye_tree_eyeglasses  \n",
    "    <li> haarcascade_mcs_leftear\n",
    "    <li> haarcascade_eye                  \n",
    "    <li> haarcascade_mcs_lefteye\n",
    "    <li> haarcascade_frontalface_alt2   \n",
    "    <li> haarcascade_mcs_mouth\n",
    "    <li> haarcascade_frontalface_alt_tree\n",
    "    <li> haarcascade_mcs_nose\n",
    "    <li> <font style=\"color: #be2830\">haarcascade_frontalface_alt</font>       \n",
    "    <li> haarcascade_mcs_rightear\n",
    "    <li> haarcascade_frontalface_default\n",
    "</ul>\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%\">\n",
    "<ul>\n",
    "    <li> haarcascade_mcs_righteye\n",
    "    <li> haarcascade_fullbody            \n",
    "    <li> haarcascade_mcs_upperbody\n",
    "    <li> haarcascade_lefteye_2splits    \n",
    "    <li> haarcascade_profileface\n",
    "    <li> haarcascade_lowerbody            \n",
    "    <li> haarcascade_righteye_2splits\n",
    "    <li> haarcascade_mcs_eyepair_big     \n",
    "    <li> haarcascade_smile\n",
    "    <li> haarcascade_mcs_eyepair_small\n",
    "    <li> haarcascade_upperbody\n",
    "</ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\" style='color: #be2830'>Detecting Faces with OpenCV</h2>\n",
    "\n",
    "``` python\n",
    "detector = cv2.CascadeClassifier( xml_file_path)\n",
    "\n",
    "face_coord = detector.detectMultiScale(image, scale_factor, min_neighbors, min_size, flags)\n",
    "```\n",
    "face_coord: Numpy array with rows equal to [x, y, width, height]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Capture and display picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "webcam = cv2.VideoCapture(0)\n",
    "_, frame = webcam.read()\n",
    "webcam.release()\n",
    "plt_show(frame)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Try to detect the face. What is the returned object?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "detector = cv2.CascadeClassifier(\"xml/frontal_face.xml\")\n",
    "\n",
    "scale_factor = 1.2\n",
    "min_neighbors = 5\n",
    "min_size = (30, 30)\n",
    "biggest_only = True\n",
    "flags = cv2.CASCADE_FIND_BIGGEST_OBJECT | \\\n",
    "            cv2.CASCADE_DO_ROUGH_SEARCH if biggest_only else \\\n",
    "            cv2.CASCADE_SCALE_IMAGE\n",
    "        \n",
    "faces_coord = detector.detectMultiScale(frame,\n",
    "                                        scaleFactor=scale_factor,\n",
    "                                        minNeighbors=min_neighbors,\n",
    "                                        minSize=min_size,\n",
    "                                        flags=flags)\n",
    "print \"Type: \" + str(type(faces_coord))\n",
    "print faces_coord\n",
    "print \"Length: \" + str(len(faces_coord)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Draw a rectangle around the face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "for (x, y, w, h) in faces_coord:\n",
    "    cv2.rectangle(frame, (x, y), (x + w, y + h), (150, 150, 0), 8)\n",
    "plt_show(frame) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Helpful classes definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class FaceDetector(object):\n",
    "    def __init__(self, xml_path):\n",
    "        self.classifier = cv2.CascadeClassifier(xml_path)\n",
    "    \n",
    "    def detect(self, image, biggest_only=True):\n",
    "        scale_factor = 1.2\n",
    "        min_neighbors = 5\n",
    "        min_size = (30, 30)\n",
    "        biggest_only = True\n",
    "        flags = cv2.CASCADE_FIND_BIGGEST_OBJECT | \\\n",
    "                    cv2.CASCADE_DO_ROUGH_SEARCH if biggest_only else \\\n",
    "                    cv2.CASCADE_SCALE_IMAGE\n",
    "        faces_coord = self.classifier.detectMultiScale(image,\n",
    "                                                       scaleFactor=scale_factor,\n",
    "                                                       minNeighbors=min_neighbors,\n",
    "                                                       minSize=min_size,\n",
    "                                                       flags=flags)\n",
    "        return faces_coord"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Helpful classes definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "class VideoCamera(object):\n",
    "    def __init__(self, index=0):\n",
    "        self.video = cv2.VideoCapture(index)\n",
    "        self.index = index\n",
    "        print self.video.isOpened()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.video.release()\n",
    "    \n",
    "    def get_frame(self, in_grayscale=False):\n",
    "        _, frame = self.video.read()\n",
    "        if in_grayscale:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        return frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Detect Face in a Live Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "webcam = VideoCamera()\n",
    "detector = FaceDetector(\"xml/frontal_face.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        frame = webcam.get_frame()\n",
    "        faces_coord = detector.detect(frame)\n",
    "        for (x, y, w, h) in faces_coord:\n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), \n",
    "                          (150, 150, 0), 8)\n",
    "        plt_show(frame) \n",
    "        clear_output(wait = True)\n",
    "except KeyboardInterrupt:\n",
    "     print \"Live Video Interrupted\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Congrats, you have learned how to detect faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\" style='color: #be2830'>Image Normalization</h2>\n",
    "\n",
    "<div style=\"float: right; width: 30%; margin-right: 100px\">\n",
    "    <img style=\"width: 50%\" src=\"https://encrypted-tbn3.gstatic.com/images?q=tbn:ANd9GcQYNNV8kaI_AeCTzO1yZFGLcHJlUU8eiQX78Fa-S39jtC0FyU56\">\n",
    "    <img src=\"http://docs.opencv.org/3.1.0/equalization_opencv.jpg\">\n",
    "</div>\n",
    "\n",
    "- Cut the Face\n",
    "- Normallize Pixel Intensity\n",
    "- Resize Face Image\n",
    "- Align Face Image? \n",
    "\n",
    "Before feeding the faces to train the model and before trying to recognize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<h2 align=\"center\" style='color: #be2830'>State of the Art - Facebook DeepFace</h2>\n",
    "\n",
    "<img style=\"float:right; width: 50%\" src=\"images/facebook_norm.png\">\n",
    "<br>\n",
    "<p>\"DeepFace: Closing the Gap to Human-Level Performance in Face Verification\"</p>\n",
    "<a href=\"http://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Taigman_DeepFace_Closing_the_2014_CVPR_paper.pdf\"><p> Y. Taigman et al., 2014.</p></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Cut the face\n",
    "<img style=\"width: 40%; float: right; margin-right: 100px\" src=\"images/cut_face.png\">\n",
    "Only the 70% of the width.\n",
    "```python\n",
    "w_rm = int(0.2 * w / 2)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def cut_faces(image, faces_coord):\n",
    "    faces = []\n",
    "      \n",
    "    for (x, y, w, h) in faces_coord:\n",
    "        w_rm = int(0.2 * w / 2)\n",
    "        faces.append(image[y: y + h, x + w_rm: x + w - w_rm])\n",
    "         \n",
    "    return faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Detect and cut the face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        frame = webcam.get_frame()\n",
    "        faces_coord = detector.detect(frame)\n",
    "        if len(faces_coord):\n",
    "            faces = cut_faces(frame, faces_coord)\n",
    "            plt_show(faces[0])\n",
    "            clear_output(wait = True)\n",
    "except KeyboardInterrupt:\n",
    "     print \"Live Video Interrupted\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Normalize Pixel Intensity\n",
    "\n",
    "<img style=\"width: 40%; float: right; margin-right: 100px\" src=\"images/histogram.png\">\n",
    "\n",
    "- The complete grayscale spectrum is used [0-255].\n",
    "- Contrast is enhanced.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_intensity(images):\n",
    "    images_norm = []\n",
    "    for image in images:\n",
    "        is_color = len(image.shape) == 3 \n",
    "        if is_color:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        images_norm.append(cv2.equalizeHist(image))\n",
    "    return images_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Equalize Histogram of last face recognized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "face_bw = cv2.cvtColor(faces[0], cv2.COLOR_BGR2GRAY)\n",
    "face_bw_eq = cv2.equalizeHist(face_bw)\n",
    "plt_show(np.hstack((face_bw, face_bw_eq)), \"Before    After\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Histogram comparison "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, sharex=True, sharey=False)\n",
    "ax1.hist(face_bw.flatten(),100)\n",
    "ax2.hist(face_bw_eq.flatten(),100, color = 'r')\n",
    "ax1.set_xlim([0,255])\n",
    "# ax1.set_ylim([0,1000])\n",
    "ax2.set_xlim([0, 255])\n",
    "# ax2.set_ylim([0, 700])\n",
    "ax1.set_yticklabels([])\n",
    "ax2.set_yticklabels([])\n",
    "ax2.set_xlabel('Pixel Intensity Grayscale')\n",
    "f.subplots_adjust(hspace=0) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Detect, cut and normalize face image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        frame = webcam.get_frame()\n",
    "        faces_coord = detector.detect(frame)\n",
    "        if len(faces_coord): \n",
    "            faces = cut_faces(frame, faces_coord)\n",
    "            faces = normalize_intensity(faces)\n",
    "            plt_show(faces[0])\n",
    "            clear_output(wait = True)\n",
    "except KeyboardInterrupt:\n",
    "     print \"Live Video Interrupted\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def resize(images, size=(50, 50)):\n",
    "    images_norm = []\n",
    "    for image in images:\n",
    "        if image.shape < size:\n",
    "            image_norm = cv2.resize(image, size, \n",
    "                                    interpolation = cv2.INTER_AREA)\n",
    "        else:\n",
    "            image_norm = cv2.resize(image, size, \n",
    "                                    interpolation = cv2.INTER_CUBIC)\n",
    "        images_norm.append(image_norm)\n",
    "\n",
    "    return images_norm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Detect, cut, normalize and resize face image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        frame = webcam.get_frame()\n",
    "        faces_coord = detector.detect(frame)\n",
    "        if len(faces_coord):\n",
    "            faces = cut_faces(frame, faces_coord)\n",
    "            faces = normalize_intensity(faces)\n",
    "            faces = resize(faces)\n",
    "            plt_show(faces[0])\n",
    "            clear_output(wait = True)\n",
    "except KeyboardInterrupt:\n",
    "     print \"Live Video Interrupted\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Alignment\n",
    "\n",
    "<p>Detect face landmarks and align face.</p>\n",
    "Example: Use eye detector to rotate image.\n",
    "\n",
    "<img style=\"height: 250px; float: left; margin-left:100px\" src=\"http://opencv-python-tutroals.readthedocs.org/en/latest/_images/face.jpg\">\n",
    "\n",
    "\n",
    "<img style=\"height: 250px; float: right; margin-right:100px\" src=\"http://blog.drndos.sk/wp-content/uploads/2014/05/11971025451307781503johnny_automatic_normal_face.svg_.hi_.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Extra functions: Bringing all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_faces(frame, faces_coord):\n",
    "    faces = cut_faces(frame, faces_coord)\n",
    "    faces = normalize_intensity(faces)\n",
    "    faces = resize(faces)\n",
    "    return faces\n",
    "\n",
    "def draw_rectangle(image, coords):\n",
    "    for (x, y, w, h) in coords:\n",
    "        w_rm = int(0.2 * w / 2) \n",
    "        cv2.rectangle(image, (x + w_rm, y), (x + w - w_rm, y + h), \n",
    "                              (150, 150, 0), 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h2 align=\"center\" style='color: #be2830'>Build Our Dataset</h2>\n",
    "<h4 align=\"center\">\n",
    "Detect $\\rightarrow$ Cut $\\rightarrow$ Normalize $\\rightarrow$ Resize $\\rightarrow$ Save</h4>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "folder = \"people/\" + raw_input('Person: ').lower() # input name\n",
    "cv2.namedWindow(\"PyData Tutorial\", cv2.WINDOW_AUTOSIZE)\n",
    "if not os.path.exists(folder):\n",
    "    os.mkdir(folder)\n",
    "    counter = 1\n",
    "    timer = 0\n",
    "    while counter < 21 : # take 20 pictures\n",
    "        frame = webcam.get_frame()\n",
    "        faces_coord = detector.detect(frame) # detect\n",
    "        if len(faces_coord) and timer % 700 == 50: # every Second or so\n",
    "            faces = normalize_faces(frame, faces_coord) # norm pipeline\n",
    "            cv2.imwrite(folder + '/' + str(counter) + '.jpg', faces[0])\n",
    "            plt_show(faces[0], \"Images Saved:\" + str(counter))\n",
    "            clear_output(wait = True) # saved face in notebook\n",
    "            counter += 1\n",
    "        draw_rectangle(frame, faces_coord) # rectangle around face\n",
    "        cv2.imshow(\"PyData Tutorial\", frame) # live feed in external\n",
    "        cv2.waitKey(50)\n",
    "        timer += 50\n",
    "    cv2.destroyAllWindows()\n",
    "else:\n",
    "    print \"This name already exists.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "del webcam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## NEXT\n",
    "<ol> \n",
    "    <h2> <li> Manipulation of Images and Videos [DONE]</h2> \n",
    "    <h2> <li> Face Detection and Building the Dataset [DONE]</h2>\n",
    "    <h2 style='color: #be2830'><a style='color: #be2830' href=\"http://localhost:8888/notebooks/03_Building_the_Recognition_Model.ipynb\"> <li> Building the Recognition Model.</a></h2>\n",
    "    <h2> <li> Recognize Faces in a Live VIdeo Feed</h2>\n",
    "<ol>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
